\section{Definition of Objectives for a Solution}\label{sec:objectForSolution}

%On the basis of the results of the previous section, the identified problems to be solved by the artifact are a gap in behavioral research in the field of data analytics and a gap in the number of experiments conducted in this field. At the same time, applications that are designed to be used to implement experiments have large capability gaps. For this reason, the artifact is designed to be implemented in the form of a software application which enables experimental research in the field of behavioral research in data analytics. In order to define an objective for a possible solution in form of a software application requirements must be engineered (\cite{Seacord.2003}). Requirements can be classified according to ISO/IEC 25000, respectively the quality model from ISO/IEC 25010, as quality criteria for software and systems.(\cite{ISOIEC25010.2011}). The \ac{ieee} defines requirements as:
%\begin{quote}
%    \textbf{\textit{\enquote{(1) A condition or capability needed by a user to solve a problem or achieve an objective. (2) A condition or capability that must be met or possessed by a system or system component to satisfy a contract, standard, specification, or other formally imposed documents. (3) A documented representation of a condition or capability as in (1) or (2). See also: design requirement; functional requirement; implementation requirement; interface requirement; performance requirement; physical equirement.}}} \cite{IEEE.1990}
%\end{quote}  
%According to these definitions, requirements can be generally defined as properties that need to be met in order to achieve an objective. In order to engineer and to provide a certain quality these requirements are established using the \textit{Requirement Engineering} approach for analysis and evaluation of requirements is utilized (\cite{SWEBOK.2004}, \cite{Sommerville.2011}). This approach has been shown to clearly contribute to software project successes in the past (\cite{Hofmann.2001}) and is therefore a suitable approach to define the objectives for a solution. The exact individual phases and steps of the \textit{Requirement Engineering} approach can vary from source to source and use case to use case. In general, however, all steps fall into one of three main categories, the \textit{Requirements Elicitation}, \textit{Requirements Specification} and \textit{Requirements Validation} (\cite{SWEBOK.2004}, \cite{Sommerville.2011}, \cite{Fernandes.2009}). In the first step, possible requirements and use cases are collected via a variety of sources like analyses, surveys, literature or interviews (\cite{Sommerville.2011}). This thesis utilizes a literature review in order to discover requirements. In the next step, the requirements are then specified and categorized. An important distinction being the difference between functional and non-functional requirements. In the \textit{Requirements Validation} step, the elicited requirements are then tested for their validity. This phase emphasis the reviewing of the requirements in order to find out whether these requirements are actually representive of the desired artefact (\cite{Sommerville.2011}). This is accomplished througt Validity, Consistency, Completeness, Realism, and Verifiability checks in conjunction with protoyping and testing the requirements (\cite{Sommerville.2011}).

On the basis of the results of the previous section, the identified problems to be solved by the artifact are a gap in behavioral research in the field of data analytics and a gap in the number of experiments conducted in this field. At the same time, applications that are designed to be used to implement experiments have large capability gaps. For this reason, the artifact is designed to be implemented in the form of a software application that enables experimental research in the field of behavioral research in data analytics. In order to define an objective for a possible solution in the form of a software application, requirements must be engineered (\cite{Seacord.2003}). Requirements can be classified according to ISO/IEC 25000, respectively, the quality model from ISO/IEC 25010, as quality criteria for software and systems (\cite{ISOIEC25010.2011}). The \ac{ieee} defines requirements as:
\begin{quote}
    \textbf{\textit{\enquote{(1) A condition or capability needed by a user to solve a problem or achieve an objective. (2) A condition or capability that must be met or possessed by a system or system component to satisfy a contract, standard, specification, or other formally imposed documents. (3) A documented representation of a condition or capability as in (1) or (2).}}} (\cite{IEEE.1990})
\end{quote}  
According to these definitions, requirements can be generally defined as properties that need to be met in order to achieve an objective. In order to engineer and provide a certain quality, these requirements are established using the \textit{Requirement Engineering} approach for the analysis and evaluation of requirements (\cite{SWEBOK.2004}, \cite{Sommerville.2011}). This approach has been shown to clearly contribute to software project successes in the past (\cite{Hofmann.2001}) and is, therefore, a suitable approach to define the objectives for a solution. The exact individual phases and steps of the \textit{Requirement Engineering} approach can vary from source to source and use case to use case. In general, however, all steps fall into one of three main categories: \textit{Requirements Elicitation}, \textit{Requirements Specification}, and \textit{Requirements Validation} (\cite{SWEBOK.2004}, \cite{Sommerville.2011}, \cite{Fernandes.2009}). In the first step, possible requirements and use cases are collected via a variety of sources like analyses, surveys, literature, or interviews (\cite{Sommerville.2011}). This thesis utilizes a literature review in order to discover requirements. In the next step, the requirements are then specified and categorized, with an important distinction being the difference between functional and non-functional requirements. In the \textit{Requirements Validation} step, the elicited requirements are then tested for their validity. This phase emphasizes the reviewing of the requirements to find out whether these requirements are actually representative of the desired artifact (\cite{Sommerville.2011}). This is accomplished through Validity, Consistency, Completeness, Realism, and Verifiability checks in conjunction with prototyping and testing the requirements (\cite{Sommerville.2011}).

%\subsection{Requirements Elicitation}

%This phase gathers information in order to discorver possible requirements for the final artefact. These requirements are discovered through a literature review of studies that have utilized experiments in the field of behavioral research in data analyitics or that could have allowed for the usage of experimental research. Moreover, other sources and the findings from the already analyzed applications for conducting experiments are also taken into account in identifying the requirements.

%\subsubsection{Studies in Data Analytics - A Literature Review }\label{subsec:literature_review_requirements}

%In order to establish further requirements a second literature review is conducted, which focusses on articles and studies in the field of behavioral research in data analytics. The goal of this second literature review is to understand commonalities and challenges of studies and especially experiments conducted in the area of data analytics, in order to establish requirements for the creation of the artifact. Due to the assumption that data analytics lies both in the fields of information systems and business administration in addition to some others the same databases as for the literature review in section \ref{sec:identification_of_the_problem} were used.

%By using the same databases as in section \ref{sec:identification_of_the_problem}, it is also ensured that the requirements are elicited based on the same general selection of literature as the original problem was identified with. The approach to the literature search, established in section \ref{sec:identification_of_the_problem}, was also used for this literature review, ensuring a thorough examination of relevant research while avoiding unnecessary altering the process of finding literature. Therefore, to maintain the integrity of the identified literature and to avoid repetition, a process was followed as described in the previous literature review (section \ref{sec:identification_of_the_problem}). Specifically, publications from journals were selectively considered, including those listed in the \textit{Senior Scholars' Basket of Journals} for Information Systems and the \textit{UT Dallas Top 100 Business School Research Rankings} for Business Administration. 

%As for the previous literature review, in order to ensure the quality of the publications, only peer-reviewed articles were considered, while book reviews, editorials, and opinion pieces were excluded. In addition, \enquote{non-scientific} texts or publications that did not meet scientific criteria were excluded from the search. The research was further refined by carefully reviewing article abstracts, which ensured that the selected literature remained relevant to the topic of experimental research in data analytics. The abstracts were reviewed not only for the use of experiments in the study, but also for research designs that could have allowed hypothesis testing by experiment. This was done to counteract the effect of selection bias. Selection bias generally describes the effects of making assumption based on a sample size, which does not represent the full population (\cite{Heckman.2010}). A simple example of Selection Bias would be to calculate the average Disposable Income of families based on the annual tax bill. This experimental design would reduce the total population to taxpaying families and thus lead to potentially grossly inaccurate results, since families living below the taxable threshold would not be included in the study. For the same principle, studies in which no experiment was performed are included in the literature review. The goal of this thesis is to improve the process of experimental research in the field of data analytics in general. Considering only studies that already perform experiments would not be representative of the full field and would therefore be prone to selection bias. It could be, for example, that certain circumstances, possibly the lack of an appropriate application, make it difficult to perform experiments. This fact would be completely lost if only studies that already perform experiments were considered. An adequate example of an article that falls into this category is Sebastian Krakowski, Johannes Luger and Sebastian Raisch's 2022 article \enquote{Artificial intelligence and the changing sources of competitive advantage}, in which they research how atrificial intelligence change the compatitive advantage by being substitues to humans in managerial tasks and decision making. For this purpose, they are examining data from chess tournaments that have already been held. However, the same research question could have been answered by conducting experiments with chess players instead of using historical data from tournaments. Remarkably, the authors seem to come to a similair conclusion, stayting further research in this area should be conducted through experiments (\cite{Krakowski.2022}). Furthermore, it should be noted that the focus of this literature review was not to outline the current state of research, but to identify as many appropriate articles as possible. For this reason, fewer search terms were used and articles from different subareas were generally admitted, as long as they are located in the larger context of data analytics or decision making. For this reason, a backwards and forwards search was also omitted.
%The search terms used for this literature review are \enquote{Data Analytics}, \enquote{Decision Making} and \enquote{Big Data}. With these terms, the search process should be kept as broad as possible. The search was furhter limited to articles written in the english language. The abstracts of all articles were then analyzed. In this way, a total of 19,955 articles were considered by inspecting their titles and, if applicable their abstracts. A full list of databases used and identified articles in their corresponding database can be found in appendix \ref{appendix:B}

%This process initially yielded 46 research publications, which were supplemented by articles from the previous literature review corresponding to the criteria. This resulted in a total number of 56 articles. The results were then classified according to the research method they utilize and whether a qualitative or quantitative approach was used. The results of this classification are presented in table \ref{tab:secondLiteratureSearch}. An important note at this point is that the number of articles using experiments does not contradict the gap identified in section \ref{sec:identification_of_the_problem}, since in this literature search specifically filtered for research articles that use experiments. Thereafter, the exact experimental setup of the articles was analyzed in order to discover requirements for their individual experiments and therefore for the artefact which is subject of thesis.

\subsection{Requirements Elicitation}

This phase gathers information in order to discover possible requirements for the final artifact. These requirements are discovered through a literature review of studies that have utilized experiments in the field of behavioral research in data analytics or that could have allowed for the usage of experimental research. Moreover, other sources and the findings from the already analyzed applications for conducting experiments are also taken into account in identifying the requirements.

\subsubsection{Literature Review of Studies in Data Analytics}\label{subsec:literature_review_requirements}

In order to establish further requirements, a second literature review is conducted, which focuses on articles and studies in the field of behavioral research in data analytics. The goal of this second literature review is to understand commonalities and challenges of studies and especially experiments conducted in the area of data analytics, in order to establish requirements for the creation of the artifact. Due to the assumption that data analytics lies in both the fields of information systems and business administration, the same databases, as used for the literature review in Section \ref{sec:identification_of_the_problem}, in addition to some others, were employed. By using the same databases as in Section \ref{sec:identification_of_the_problem}, it is also ensured that the requirements are elicited based on the same general selection of literature as the original problem was identified with. The approach to the literature search, established in Section \ref{sec:identification_of_the_problem}, was also used for this literature review, ensuring a thorough examination of relevant research while avoiding unnecessary alterations to the process of finding literature. Therefore, to maintain the integrity of the identified literature and to avoid repetition, a process was followed as described in the previous literature review (Section \ref{sec:identification_of_the_problem}). Specifically, publications from journals were selectively considered, including those listed in the \textit{Senior Scholars' Basket of Journals} for Information Systems and the \textit{UT Dallas Top 100 Business School Research Rankings} for Business Administration. As for the previous literature review, in order to ensure the quality of the publications, only peer-reviewed articles were considered, while book reviews, editorials, and opinion pieces were excluded. In addition, \enquote{non-scientific} texts or publications that did not meet scientific criteria were excluded from the search. The research was further refined by carefully reviewing article abstracts, which ensured that the selected literature remained relevant to the topic of experimental research in data analytics. The abstracts were reviewed not only for the use of experiments in the study but also for research designs that could have allowed hypothesis testing by experiment. This was done to counteract the effect of selection bias. Selection bias generally describes the effects of making assumptions based on a sample size that does not represent the full population (\cite{Heckman.2010}). A simple example of selection bias would be calculating the average disposable income of families based on the annual tax bill. This experimental design would reduce the total population to taxpaying families and thus lead to potentially grossly inaccurate results, since families living below the taxable threshold would not be included in the study. For the same principle, studies in which no experiment was performed are included in the literature review. The goal of this thesis is to improve the process of experimental research in the field of data analytics in general. Considering only studies that already perform experiments would not be representative of the full field and would therefore be prone to selection bias. It could be, for example, that certain circumstances, possibly the lack of an appropriate application, make it difficult to perform experiments. This fact would be completely lost if only studies that already perform experiments were considered. An adequate example of an article that falls into this category is Sebastian Krakowski, Johannes Luger, and Sebastian Raisch's 2022 article \enquote{Artificial intelligence and the changing sources of competitive advantage,} in which they research how \ac{ai} changes the competitive advantage by being substitutes for humans in managerial tasks and decision making. For this purpose, they are examining data from chess tournaments that have already been held. However, the same research question could have been answered by conducting experiments with chess players instead of using historical data from tournaments. Remarkably, the authors seem to come to a similar conclusion, stating that further research in this area should be conducted through experiments (\cite{Krakowski.2022}). Furthermore, it should be noted that the focus of this literature review was not to outline the current state of research but to identify as many appropriate articles as possible. For this reason, fewer search terms were used, and articles from different subareas were generally admitted, as long as they are located in the larger context of data analytics or decision making. For this reason, a backward and forward search was also omitted.
The search terms used for this literature review are \enquote{Data Analytics}, \enquote{Decision Making}, and \enquote{Big Data}. With these terms, the search process should be kept as broad as possible. The search was further limited to articles written in the English language. The abstracts of all articles were then analyzed. In this way, a total of 19,955 articles were considered by inspecting their titles and, if applicable, their abstracts. A full list of databases used and identified articles in their corresponding database can be found in Appendix \ref{appendix:B}.

\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{ll}
    \hline
    \multicolumn{1}{l}{Research Method} & \multicolumn{1}{l}{Total} \\ \hline
    \multicolumn{1}{l}{Case  Study}     & \multicolumn{1}{c}{16}    \\ 
    \multicolumn{1}{l}{Interview}       & \multicolumn{1}{c}{3}     \\ 
    \multicolumn{1}{l}{Experiment}      & \multicolumn{1}{c}{10}    \\ 
    \multicolumn{1}{l}{Observation}     & \multicolumn{1}{c}{3}     \\ 
    \multicolumn{1}{l}{Survey}          & \multicolumn{1}{c}{17}    \\ 
    \multicolumn{1}{l}{Data Analysis}   & \multicolumn{1}{c}{7}     \\ 
                                            &                            \\ \hline
    \multicolumn{1}{l}{Quantitative}    & \multicolumn{1}{c}{34}    \\ 
    \multicolumn{1}{l}{Qualitative}     & \multicolumn{1}{c}{32}    \\ \hline
    \end{tabular}
    \caption[Research Methods for Literature Search on Requirements]{Research Methods for Literature Search on Requirements}\label{tab:secondLiteratureSearch}
    \end{table}

This process initially yielded 46 research publications, which were supplemented by articles from the previous literature review corresponding to the criteria. This resulted in a total number of 56 articles. The results were then classified according to the research method they utilize and whether a qualitative or quantitative approach was used. The results of this classification are presented in Table \ref{tab:secondLiteratureSearch}. An important note at this point is that the number of articles using experiments does not contradict the gap identified in Section \ref{sec:identification_of_the_problem}, since in this literature search specifically filtered for research articles that use experiments. Thereafter, the exact experimental setup of the articles was analyzed in order to discover requirements for their individual experiments and therefore for the artifact which is the subject of this thesis.




%The requirements derived from this are included in Table \ref{tab:requirementsRaw1}. As already mentioned, not only the requirements of the performed experiments were included, but also the requirements of setups that would have been suitable for an experiment. The column \enquote{Proper} indicates whether a requirement was actually mentioned directly in the article, and the column \enquote{Extra} indicates whether this requirement would have had to be used theoretically if the study had conducted an experiment. As already explained, this should ensure that not only requirements that fit already performed experiments are discovered, but also to counteract selection bias.

\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{lcclccc}
    \hline
    Requirement          & \multicolumn{1}{l}{Proper} & \multicolumn{1}{l}{Extra} & Requirement & \multicolumn{1}{l}{Proper} & \multicolumn{1}{l}{Extra} \\ \hline
    Reusable                                                  & 11                         & 0                         & Participant Data                                           & 24                         & 23                        \\
    Interoperability                                           & 1                          & 0                         & Displaying Information                                     & 22                         & 23                        \\
    Meta Data Collecting                                       & 8                          & 0                         & Different Groups                                           & 14                         & 2                         \\
    Post-Interview                                             & 2                          & 0                         & Additional Logic                                           & 12                         & 0                         \\
    Time-Flexibility                                           & 9                          & 0                         & Evaluation of Data                                         & 27                         & 22                        \\
    Multi-Source                                               & 1                          & 0                         & Participant Input                                          & 21                         & 25                        \\
    Vizualize Final Data                                       & 2                          & 0                         & Real-Time Exchange                                         & 3                          & 0                         \\
    Proactive System                                           & 2                          & 0                         & Distant Communication                                      & 18                         & 0                         \\
    Pre-Loading Data                                           & 1                          & 0                         & Selecting Data                                              & 2                          & 0                         \\
    Monitoring of Study                                        & 1                          & 0                         & Simplicity                                                  & 1                          & 0                         \\
    Debrefing Info                                             & 1                          & 0                         & & & & \\\hline
    \end{tabular}
    \caption[Requirements Uncatagorized]{Requirements Uncatagorized}\label{tab:requirementsRaw1}
    \end{table}


The requirements derived from this are included in Table \ref{tab:requirementsRaw1}. As already mentioned, not only the requirements of the performed experiments were included, but also the requirements of setups that would have been suitable for an experiment. The column \enquote{Proper} indicates whether a requirement was actually mentioned directly in the article, and the column \enquote{Extra} indicates whether this requirement would have had to be used theoretically if the study had conducted an experiment. As already explained, this should ensure that not only requirements that fit already performed experiments are discovered, but also to counteract selection bias.


 % \subsubsection{Further relevant reference resources}\label{subsec:furtherRequirements}

 % In addition to the literature review, this section includes other sources that can be derived into requirements. These sources and requirements mainly refer to external influencing factors or specifications. Only requirements that have not yet been established by the previous methods are included.  
 % In their book on Empirical Educational Research, H. Reinders,H. Ditton and C. Gr√§sel describe, among other things, the structure and empirical theory in relation to experiments. An important part of conducting experiments, according to the authors, is to educate the subjects about the experiment and its benefits. Rather than explaining the design of the experiment, this involves explaining the actual benefits and goal of the experiment to the participants after it has been conducted (\cite{Gniewosz.2011}). Furthermore, confounding variables have to be eliminated for the effective execution of experiments. Although some of these measures must be implemented on a case-by-case basis by the individual experimental setup itself, the artifact is intended to assist in this process when possible. Especially measures regarding the allocation of groups represent an important part of experiments (\cite{Gniewosz.2011}) and could be supported by an artefact. For this reason, the requirements \enquote{Random or targeted assignment of test subjects} are included in the list of requirements. The creation of control groups via different participant groups is already included as a requirement from several sources. In addition, findings from the analysis of the alternative applications for conducting experiments from Section \ref{sec:identification_of_the_problem} are used to establish requirements. These include the use of complex and up to date user interfaces, the deployment on non windows platforms, the open-source approach and the possibility to add own program code for the experiments based on the analysis of z-Tree and oTree. Further requirements based on the analysis of Lionness Lab are openness of the platform, the creation of different groups regarding the test subjects and the possibility to use unrestricted coding.

%\subsection{Requirements Specification}\label{subsec:reqSpec}

%After the requirements have been discovered and roughly outlined in the previous section, they are concretely specified, organized and classified in this section. For this purpose, the requirements were specified concretely with explaination and categorized based on their task. At the same time, the requirements were classified into functional and non-functional requirements. A functional requirement describes a function that a system or system component must be able to perform (\cite{IEEE.1990}). An example of a functional requirement would be the calculation of a pricetag in euros and in dollars. Non-functional characteristics describe on the other hand the behavior of a system (\cite{Seacord.2003}) and go thereby beyond the functional characteristics. Thus functional requirements describe what a system must be able to do and non-functional requirements describe how this should be done. Non-functional requirements also often describe the quality of the individual functions and can influence several other requirements (\cite{Balzert.2011}). An example of non-functional properties would be that the conversion from euros to dollars must be performed in \enquote{a few seconds}. Table \ref{tab:FuncRequirementsCat} contains the functional and Table \ref{tab:NonFuncRequirementsCat} the non-functional requirements. The requirements are composed of the requirements discovered in sections \ref{subsec:literature_review_requirements} and \ref{subsec:furtherRequirements} and are included together in the tables. Requirements derived from multiple sources and methods are also just listed once in the table. 

\subsubsection{Further Relevant Reference Resources}\label{subsec:furtherRequirements}

In addition to the literature review, this section includes other sources that can contribute to the derivation of requirements. These sources and requirements mainly refer to external influencing factors or specifications. Only requirements that have not yet been established by the previous methods are included.

In their book on Empirical Educational Research, Gniewosz et al. (2011) describe, among other things, the structure and empirical theory in relation to experiments. An important part of conducting experiments, according to the authors, is to educate the subjects about the experiment and its benefits. Rather than explaining the design of the experiment, this involves explaining the actual benefits and goals of the experiment to the participants after it has been conducted (\cite{Gniewosz.2011}). Furthermore, confounding variables have to be eliminated for the effective execution of experiments. Although some of these measures must be implemented on a case-by-case basis by the individual experimental setup itself, the artifact is intended to assist in this process when possible. Especially measures regarding the allocation of groups represent an important part of this (\cite{Gniewosz.2011}) and could be supported by an artifact. For this reason, the requirements to assign test subjects to groups randomly or targeted are included in the list of requirements. The creation of control groups via different participant groups is already included as a requirement from several sources. In addition, findings from the analysis of the alternative applications for conducting experiments from Section \ref{sec:identification_of_the_problem} are used to establish requirements. These include the use of complex and up-to-date user interfaces, deployment on non-Windows platforms, the open-source approach, and the possibility to add custom program code for the experiments based on the analysis of z-Tree and oTree. Further requirements based on the analysis of LIONESS Lab include the openness of the platform, the creation of different groups regarding the test subjects, and the possibility to use custom coding.

\subsection{Requirements Specification}\label{subsec:reqSpec}

After the requirements have been discovered and roughly outlined in the previous section, they are concretely specified, organized, and classified in this section. For this purpose, the requirements were specified concretely with explanations and categorized based on their task. At the same time, the requirements were classified into functional and non-functional requirements. A functional requirement describes a function that a system or system component must be able to perform (\cite{IEEE.1990}). An example of a functional requirement would be the calculation of a price tag in euros and in dollars. Non-functional characteristics describe, on the other hand, the behavior of a system (\cite{Seacord.2003}) and go beyond functional characteristics. Thus, functional requirements describe what a system must be able to do, and non-functional requirements describe how this should be done. Non-functional requirements also often describe the quality of the individual functions and can influence several other requirements (\cite{Balzert.2011}). An example of non-functional properties would be that the conversion from euros to dollars must be performed in \enquote{a few seconds}. Table \ref{tab:FuncRequirementsCat} contains the functional, and Table \ref{tab:NonFuncRequirementsCat} the non-functional requirements. The requirements are composed of the requirements discovered in Sections \ref{subsec:literature_review_requirements} and \ref{subsec:furtherRequirements} and are included together in the tables. Requirements derived from multiple sources and methods are also just listed once in the table.



\begin{table}
    \centering
    \small
    \begin{tabular}{L{0.35\textwidth}L{0.59\textwidth}}
    \hline
    Requirement                     & Description \\ \hline
    \rowcolor[HTML]{C0C0C0} 
    Information                 &             \\ \hline
    F1.1 Displaying Information     & Information must be able to be displayed            \\
     & \\
    F1.2 Debrefing Info             & Debrifing Information must be able to be displayed           \\
    & \\ \hline
    \rowcolor[HTML]{C0C0C0} 
    Data Collecting             &             \\ \hline
    F2.1 Participant Data           & Basic data about the participants must be able to be collected           \\
    F2.2 Meta-Data                  & Meta data must be collected            \\
    & \\
    F2.3 Post-Interview             & It must be possible to collect data after the experiment            \\
    & \\  \hline
    \rowcolor[HTML]{C0C0C0} 
    Pre-Loading                 &             \\ \hline
    F3.1 Pre-Loading Data           & Data must be able to be pre-loaded            \\
    & \\
    F3.2 Selecting Data             & Data must be able to be pre-selected and deleted           \\
    & \\ \hline
    \rowcolor[HTML]{C0C0C0} 
    Experiment Setup            &             \\ \hline
    F4.1 Additional Logic           & Custom logic/program code can be executed within the artifact            \\
    F4.2 Participant Input          & The artifact enables user input            \\
    & \\
    F4.3 Proactive System           & The artifact can pro-actively propmt a user action           \\
    & \\ \hline
    \rowcolor[HTML]{C0C0C0} 
    Groups                      &             \\ \hline
    F5.1 Different Groups           & The artifact must allow the division of participants into different groups            \\
    F5.2 Communication of Groups    & The different groups must be able to interact with each other            \\ 
    & \\
    F5.3 Targeted Assignment        & Groups of test subjects must be able to be created based on certain attributes like confounding variables. \\
    F5.4 Random Assignment          & Groups of test subjects must be able to be created based on random assignment \\ \hline
    \end{tabular}
    \caption[Functional Requirements Structured]{Functional Requirements Structured}\label{tab:FuncRequirementsCat}
    \end{table}


\begin{table}
    \centering
    \small
    \begin{tabular}{L{0.35\textwidth}L{0.59\textwidth}}
    \hline
Requirement                             & Description \\ \hline
    \rowcolor[HTML]{C0C0C0} 
    Time-space non-reliance     &             \\ \hline
    N1.1 Distand Communication      & The artifact can be used regardless of the location of the participant            \\
    N1.2 Time-Flexibility           & The artifact can be used independently of a given period of time            \\ \hline
    \rowcolor[HTML]{C0C0C0} 
    Data Postprocessing &             \\ \hline
    N2.1 Evaluation of Data         & Data can be retrieved in a suitable way for further processing and evaluation            \\
    N2.2 Vizualize Final Data       & Data can be retrieved in a suitable way for further visualization             \\ \hline
    \rowcolor[HTML]{C0C0C0} 
    Simplicity                  &            \\ \hline
    N3.1 Simplicity                 & The artifact is simple to use           \\
    & \\ \hline
    \rowcolor[HTML]{C0C0C0} 
    Reusable and Interoperable  &             \\ \hline
    N4.1 Reusable                   & Experiments with the artifact are easy to re-do           \\
    & \\
    N4.2 Interoperability           & The artifact is interoperable            \\
    & \\
    N4.3 Openness of Platform       & The artifact is open to changes and enhancements \\
    & \\ \hline
    \rowcolor[HTML]{C0C0C0} 
    Monitoring                  &             \\ \hline
    N5.1 Monitoring of Study        & The study conducted with the artifact can be monitored            \\ 
    & \\ \hline
    \rowcolor[HTML]{C0C0C0} 
    Pre-Loading                 &             \\ \hline
    N6.1 Multi-Source             & Data from multiple-sources must be pre-loaded            \\ 
    & \\ \hline
    \rowcolor[HTML]{C0C0C0} 
    Advanced User Interface   &             \\ \hline
    N7.1 Advanced User Interface  & The Artifact enables the usage of modern user interface components            \\ \hline
    \end{tabular}
    \caption[Non-Functional Requirements Structured]{Non-Functional Requirements Structured}\label{tab:NonFuncRequirementsCat}
    \end{table}
\newpage


\subsection{Requirements Validation}\label{subsec:requirement_validation}

%In this section, the previously established requirements are validated. The criteria Validity, Consistency, Completeness, Realism, and Verifiability from the \textit{Requirement Engineering} approach are used for this purpose. The validity criterion indicates whether the requirements imposed actually correspond to the intended functions. Since the requirements were drawn up by taking into account applications that have already been developed and studies that have already been carried out, it can be assumed that the requirements correspond more precisely to the functions that are actually required than if they had been drawn up by any stakeholders. In addition, the requirements were established using a clearly defined scientific process and by adding literature from the field which should also increase validity. Some of the requirements derived from analysing existing applications are also conform with requirements from subsection \ref{subsec:literature_review_requirements} and \ref{subsec:furtherRequirements}, indicating that the requirement search covers the problem to be solved very well. For these reasons, it is assumed with a high probability that the validity of the requirements is guaranteed. Moreover, the established requirements in Table \ref{tab:FuncRequirementsCat} and \ref{tab:NonFuncRequirementsCat} do not contradict each other, which means that the consistency criterion is also met. The \enquote{completeness} criterion, which describes whether the overall scope of the functions is covered by the requirements, is difficult to confirm. The reason for this is the open nature of the artifact, which should enable the improved execution of arbitrary experiments. Thus, the functional scope of the artifact is theoretically endless. Nevertheless, this criterion can be confirmed considering requirement F4.1 ensures that additional logic and program code can be implemented within the artefact, which means that theoretically an infinite number of further functions can be implemented by the person performing the experiment (as far as these requirements can be implemented by a Turing-Complete programming language). As a result, the \enquote{Completness} criterion is also considered to be fulfilled. The criterion \enquote{realism} can also be confirmed. No requirement indicates that it could not be implemented technically as will be shown in section \ref{sec:designArtefact}. The last criterion to confirm the validity of the requirements is the \enquote{Verifiability}, which describes whether the individual requirements are formulated in a way that they can be tested. An important point, since the artefact must be tested for all requirements following the \ac{dsr} approach. In order to be able to formally confirm the criterion, test cases are therefore designed which check the artifact against the corresponding requirements. This approach is also supported by the scientific literature for the validation of requirements and is later used in the \ac{dsr} approach to confirm the requirements (\cite{Sommerville.2011}).

In this section, the previously established requirements are validated. The criteria validity, consistency, completeness, realism, and verifiability from the \textit{Requirement Engineering} approach are used for this purpose. The validity criterion indicates whether the requirements imposed actually correspond to the intended functions. Since the requirements were drawn up by taking into account applications that have already been developed and studies that have already been carried out, it can be assumed that the requirements correspond more precisely to the functions that are actually required than if they had been drawn up by any stakeholders. In addition, the requirements were established using a clearly defined scientific process and by adding literature from the field, which should also increase validity. Some of the requirements derived from analyzing existing applications are also in conformity with requirements from Subsections \ref{subsec:literature_review_requirements} and \ref{subsec:furtherRequirements}, indicating that the requirement search covers the problem to be solved very well. For these reasons, it is assumed with a high probability that the validity of the requirements is guaranteed. Moreover, the established requirements in Tables \ref{tab:FuncRequirementsCat} and \ref{tab:NonFuncRequirementsCat} do not contradict each other, which means that the consistency criterion is also met. The completeness criterion, which describes whether the overall scope of the functions is covered by the requirements, is difficult to confirm. The reason for this is the open nature of the artifact, which should enable the improved execution of arbitrary experiments. Thus, the functional scope of the artifact is theoretically endless. Nevertheless, this criterion can be confirmed considering requirement F4.1 ensures that additional logic and program code can be implemented within the artifact, which means that theoretically an infinite number of further functions can be implemented by the person performing the experiment (as far as these requirements can be implemented by a Turing-Complete programming language). As a result, the completeness criterion is also considered to be fulfilled. The criterion realism can also be confirmed. No requirement indicates that it could not be implemented technically, as will be shown in Section \ref{sec:designArtefact}. The last criterion to confirm the validity of the requirements is the verifiability, which describes whether the individual requirements are formulated in a way that they can be tested. This is an important point since the artifact must be tested for all requirements following the \ac{dsr} approach. In order to be able to formally confirm the criterion, test cases (Table \ref{tab:testVal}) are therefore designed which check the artifact against the corresponding requirements. This approach is also supported by the scientific literature for the validation of requirements and is later used in the \ac{dsr} approach to confirm the requirements (\cite{Sommerville.2011}).


\begin{table}[htbp]
    \centering
    \small
    \begin{tabular}{L{0.04\textwidth}L{0.7\textwidth}L{0.15\textwidth}}
    \hline
    ID  & Test Description                                                                                                                                     & Requirement                  \\ \hline
    T1  & A welcome and goodbye message is displayed                                                                                               & F1.1, F1.2                 \\
    & \\
    T2  & Participants are prompted to input their age at the beginning and prompted to input how the liked the experiment at the end              & F2.1, F2.3, F4.3, F4.2 \\
    & \\
    T3  & The information about how long the experiment took is collected                                                                          & F2.2                         \\
    & \\
    T4  & The gender and the weight of the participant is pre-loaded into the experiment from different files. The gender of the participant is deleted. & F3.1, F3.2, N6.1                         \\
    & \\
    T5  & A chess game is added as custom logic                                                                                                    &  F4.1, F4.2, F4.3         \\
    & \\
    T6  & Two groups are created, one of the groups is particularly chosen the other one randomly selected                                           & F5.1, F5.3, F5.4         \\
    & \\
    T7  & A chess turn is played by both parties not using the same device                                                                         & F4.2, F5.2, N1.1, N1.2         \\
    & \\
    T8  & The results of the experiment are retrieved and displayed in third party software                                                        & F2.2, N2.1, N2.2, N5.1                \\
    & \\
    T9  & The experiment is redone a second time and another experimental setup is implemented                                                     & N4.1                         \\
    & \\
    T10 & The experiment is conducted on different devices                                                                                         & N4.2                         \\
    & \\
    T11 & During the experiment the current state of the chess board is exported to the conducter of the experiment                                & N5.1                         \\ 
    & \\ \hline
    \end{tabular}
    \caption[Validation Tests for Requirements]{Validation Tests for Requirements}\label{tab:testVal}
    \end{table}

%The non-functional requirements simplicity, openness of platform and advanced user interface cannot be precisely tested by a test case due to their subjectivity. One way to counteract this would be to formulate the requirement more precisely. For example the requirement \enquote{advanced user interface} could be replaced by with \enquote{a user interface that was developed in 2023}. This would satisfy the verifiability criterion, but no longer the validity criterion, since this requirement is actually fairly subjective and the test might not capture the initial intention of the requirement. The original intention being to have an advanced user interface, which is up to date with current technologies at this specific point. This fact is the same for all of these three non-functional requirements. These are partly subjective requirements that cannot be clearly covered by test cases. Nevertheless, these requirements represent important insights and demands on the developed artifact. For this reason, the three requirements \enquote{Simplicity}, \enquote{openess of platform} and \enquote{advanced user interface} are included as valid requirements, but no test cases can be included for them. Nevertheless, they are taken into account in the development of the artefact and finally evaluated as best as possible. Regaring the other requirements, by successfully setting up tests for them, the \enquote{Verifiability} criterion can thus also be confirmed. Thus, the established requirements meet the criteria Validity, Consistency, Completeness, Realism, and Verifiability and are thus valid requirements for the artifact.

The non-functional requirements of \enquote{Simplicity}, \enquote{Openness of Platform}, and an \enquote{Advanced User Interface} cannot be precisely tested by a test case due to their subjectivity. One way to counteract this would be to formulate the requirements more precisely. For example, the requirement \enquote{Advanced User Interface} could be replaced with \enquote{A User Interface Developed in 2023}. This would satisfy the verifiability criterion but would no longer satisfy the validity criterion, since this requirement is actually fairly subjective, and the test might not capture the initial intention of the requirement. The original intention of the requirement is to have an advanced user interface that stays up to date with current technologies. The same holds true for all three of these non-functional requirements. They are partly subjective requirements that cannot be clearly covered by test cases. Nevertheless, these requirements represent important insights and demands for the developed artifact. For this reason, the three requirements \enquote{Simplicity}, \enquote{Openness of Platform}, and \enquote{Advanced User Interface} are included as valid requirements, but no test cases can be included for them. Nevertheless, they are taken into account in the development of the artifact and finally evaluated as best as possible. Regarding the other requirements, by successfully setting up tests for them, the verifiability criterion can thus also be confirmed. Thus, the established requirements meet the criteria of validity, consistency, completeness, realism, and verifiability and are thus valid requirements for the artifact.

